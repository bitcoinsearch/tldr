<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>1</id>
  <title>Why do we need a MAX_BLOCK_SIZE at all?</title>
  <updated>2023-06-09T22:04:31.160507+00:00</updated>
  <author>
    <name>Jim Phillips 2015-06-01 20:02:31</name>
  </author>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="0.9.0">python-feedgen</generator>
  <entry>
    <id>1</id>
    <title>Why do we need a MAX_BLOCK_SIZE at all?</title>
    <updated>2023-06-09T22:04:31.160507+00:00</updated>
    <link href="https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-June/008440.html" rel="alternate"/>
    <summary>In order to maintain consensus in the network, there needs to be a clearly defined rule about which blocks are valid and which are not. This is achieved by setting a limit on the maximum block size that everyone agrees on. Relays can set configurable limits on the size of blocks they will relay, preventing a rogue miner from flooding the network with large blocks. Over time, relays can raise the limits they set on block sizes they will propagate, allowing the true maximum block size to grow naturally without requiring a hard fork.Removing the 1 MB cap entirely could lead to further centralization of pools, as larger pools may be able to create larger blocks that smaller pools cannot handle. This would force smaller pools out of the network and increase the percentage of network hashrate for the original pool operator. Allowing relays to restrict propagation can mitigate this issue, as they can refuse to relay large blocks filled with nonsense or coming from a miner known for producing bad blocks. Anonymizing networks like TOR are not geared towards handling static IP traffic, so large miners who have to use static IPs would not risk getting blacklisted by relays.Larger blocks mean fewer people can download and verify the chain, resulting in fewer people willing to run full nodes and store all of the blockchain data. Artificially bloating the block with nonsense could increase server costs for everyone running a full node and make it infeasible for people with home computers to even keep up with the network. Finding a block size limit with the right tradeoff between resource restrictions and functional requirements is important, but no such solution currently exists. However, this same attack could be achieved simply by sending lots of spam transactions and bloating the UTXO database or the mempool. This particular attack vector has already been largely mitigated by pruning and could be further mitigated by allowing relays to decide which blocks they propagate.</summary>
    <published>2015-06-01T20:02:31+00:00</published>
  </entry>
</feed>
