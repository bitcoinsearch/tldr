<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>1</id>
  <title>Block size following technological growth</title>
  <updated>2023-06-10T04:33:21.551622+00:00</updated>
  <author>
    <name>Hector Chu 2015-08-04 11:34:58</name>
  </author>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="0.9.0">python-feedgen</generator>
  <entry>
    <id>1</id>
    <title>Block size following technological growth</title>
    <updated>2023-06-10T04:33:21.551622+00:00</updated>
    <link href="https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-August/009898.html" rel="alternate"/>
    <summary>The debate around block size in Bitcoin has been ongoing for several years. Many people are advocating for larger blocks despite the fact that the mining landscape is very centralized, full node counts are historically low, and there is an increasing trend of outsourcing full validation. If the majority agreed that the block size was causing problems, they would be campaigning for a reduction in block size. Hector Chu believes that a one-time increase to 8MB is the way forward, which is somewhere between Mike Hearn's extreme view of removing the block size limit altogether and the view that the block size should be artificially constrained below the organic growth curve. However, Pieter Wuille disagrees with this view, stating that Bitcoin can work even with gigabyte-sized blocks today if everyone uses the same few blockchain validation services, online wallets, and mining is done by a cartel that only allows joining after signing a contract so they can sue you if you create an invalid block. He argues that Bitcoin will become uninteresting if it becomes something like this.Jorge Tim√≥n has been asking whether a bigger consensus size can cause less mining centralization, but Venzen explains that this is a non-sequitur. Gavin Andresen ran simulations before choosing the number 20MB as a reasonable block size target because 170 gigabytes per month comfortably fits into the typical 250-300 gigabytes per month data cap, so you can run a full node from home on a pretty good broadband plan. Mike Hearn understands that the consensus block size maximum rule is a tool for limiting mining centralization, and Hector Chu agrees that not increasing centralization should be the priority when deciding about a consensus rule that limits mining centralization. However, compromise may be necessary to find common ground with others. Pieter Wuille believes that not increasing centralization should be the primary concern when raising the consensus limit, rather than the potential impact on market fees.In a discussion on the Bitcoin-dev mailing list, a participant asked whether not limiting mining centralization was due to a lack of belief in its effectiveness or simply not caring about it. The context includes links to the mailing list and a sign-up page for interested individuals. Overall, the debate around block size in Bitcoin is complex, with various opinions and considerations to take into account.</summary>
    <published>2015-08-04T11:34:58+00:00</published>
  </entry>
</feed>
