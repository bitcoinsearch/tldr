<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>1</id>
  <title>Proposed alternatives to the 20MB step function</title>
  <updated>2023-06-09T20:11:45.445261+00:00</updated>
  <author>
    <name>Bryan Bishop 2015-05-08 16:55:42</name>
  </author>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="0.9.0">python-feedgen</generator>
  <entry>
    <id>1</id>
    <title>Proposed alternatives to the 20MB step function</title>
    <updated>2023-06-09T20:11:45.445261+00:00</updated>
    <link href="https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-May/008012.html" rel="alternate"/>
    <summary>In an email conversation in 2015, Bitcoin developer Matt Whitlock suggested the hard block size limit be a function of actual block sizes over a trailing sampling period. The proposal would allow Bitcoin to scale up gradually and organically, rather than have human beings guess at what is an appropriate limit. The suggestion could help slow down the process of pushing out miners and network participants who can't handle 100 GB blocks immediately. As the size of the blocks increase, low-end hardware participants may no longer meet the minimum performance requirements and may need to fall off the network. However, there are concerns that the adjustment may become severely mismatched with general economic trends in data storage device development or availability or even current-market-saturation of said storage devices. Additionally, transaction stuffing or grinding can be used to game the 2016 block median metric to increase faster than other participants can keep up with or, perhaps worse, in a way that was unintended by developers yet known to be a failure mode.</summary>
    <published>2015-05-08T16:55:42+00:00</published>
  </entry>
</feed>
