<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>1</id>
  <title>Hard fork proposal from last week's meeting</title>
  <updated>2023-06-11T22:48:47.629678+00:00</updated>
  <author>
    <name>Jared Lee Richardson 2017-04-01 06:15:09</name>
  </author>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="0.9.0">python-feedgen</generator>
  <entry>
    <id>1</id>
    <title>Hard fork proposal from last week's meeting</title>
    <updated>2023-06-11T22:48:47.629678+00:00</updated>
    <link href="https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-April/013942.html" rel="alternate"/>
    <summary>The scalability of processing transactions in a distributed system like Bitcoin is a significant challenge. The blocksize debate in the Bitcoin community has led to discussions on how many transactions can be effectively processed by blockchains and how to handle peak volumes. Even with the proposed increase in block size, the peak volume of transactions is expected to be around 200k-500k transactions per second, which requires several experts to maintain. To scale systems to massive sizes, we must consider various factors such as RAM, storage, CPU, and bandwidth costs. However, these issues are not unsolvable, as companies already handle data storage and updates approaching hundreds of millions of datapoints per second. While clusters are more expensive to set up per-resource because they need to talk to each other and synchronize with each other, the cost structure for scaling is not necessarily in the six or seven-digit range.There are distinct cut-off points for handling transaction volumes, with each limit requiring a choice between killing off on-chain use cases versus losing nodes who can't reach the next limit effectively. Additionally, transaction processing is largely non-parallel, making it challenging to check each transaction against each other to prevent double spending. At peak transaction volumes, RAM consumption increases with large blocks versus small ones, but there are trade-offs that can be made to write to disk if RAM usage grows too large. If signature verification controls, a specialized ASIC chip may be able to verify signatures hundreds of times faster. While rebuilding everything without taking the system offline may seem daunting, businesses can tolerate three-nines uptime operation and could even hire one or two IT personnel to run the operation on the side with their extra duties.The cost of maintaining such a system is much higher than that of smaller systems due to the increased amount of data storage required and the need for redundancy and tolerance to prevent the system from crashing. The cost structures for data centers that process airline ticketing are above $5 million per year for larger airlines. Visa is likely to be even more expensive than that, given the scale of its operations. Instead of trying to fit all desired transactions on a blockchain, it's important to consider what use-cases can be enabled without sacrificing essential features. It is crucial to think critically about the challenges that come with scaling a distributed system and consider the cost implications of such a move. It is clear that dealing with mission-critical data at this scale requires a lot of expertise and clever solutions to ensure that everything works consistently.</summary>
    <published>2017-04-01T06:15:09+00:00</published>
  </entry>
</feed>
