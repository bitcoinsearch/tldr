<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>1</id>
  <title>Why are we bleeding nodes?</title>
  <updated>2023-06-08T18:39:53.694414+00:00</updated>
  <author>
    <name>Paul Lyon 2014-04-07 21:55:53</name>
  </author>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="0.9.0">python-feedgen</generator>
  <entry>
    <id>1</id>
    <title>Why are we bleeding nodes?</title>
    <updated>2023-06-08T18:39:53.694414+00:00</updated>
    <link href="https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2014-April/005108.html" rel="alternate"/>
    <summary>A user is concerned about whether asking for headers from each peer they are connected to and dumping them into the backend to be sorted out is abusive to the network. They note that they won't end up downloading the full blockchain's worth of headers from every peer, but rather get an updated view of the current winning chain before sending out additional header requests to peers. Tamas Blummer responds that loading headers sequentially is not strictly necessary, and if a user is connected to honest nodes, they could download portions of the chain and connect the various sub-chains together. He notes that other peers could be used to parallel download the blockchain while the main chain is downloading, and even if the header download stalled, it wouldn't be that big a deal. Paul Lyon shares his approach, where headers can be downloaded and stored in any order to make sense of what the winning chain is. Blocks don't need to be downloaded in any particular order and they don't need to be saved to disk; the UTXO is fully self-contained, allowing him to do the initial blockchain sync in ~6 hours when using an SSD. Validation has to be sequential, but that step can be deferred until the blocks before a point are loaded and continuous.</summary>
    <published>2014-04-07T21:55:53+00:00</published>
  </entry>
</feed>
