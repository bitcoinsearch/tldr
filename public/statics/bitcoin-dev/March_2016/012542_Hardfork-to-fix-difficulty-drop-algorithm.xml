<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>1</id>
  <title>Hardfork to fix difficulty drop algorithm</title>
  <updated>2023-06-11T04:18:03.235126+00:00</updated>
  <author>
    <name>Bob McElrath 2016-03-09 20:21:36</name>
  </author>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="0.9.0">python-feedgen</generator>
  <entry>
    <id>1</id>
    <title>Hardfork to fix difficulty drop algorithm</title>
    <updated>2023-06-11T04:18:03.235126+00:00</updated>
    <link href="https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-March/012542.html" rel="alternate"/>
    <summary>The problem of working out a timeframe over which to run the derivative calculations is not an easy one. From a measurement theory perspective, this can be done by treating each block as a measurement and performing error propagation to derive an error on the derivatives. Bitcoin's block timing follows the statistical theory of Poisson Point Process, and there is a lot of literature available on how to handle this.The measurement of the hashrate is quite inaccurate at best, which means that even with a completely constant hash rate running indefinitely, we would still see difficulty swings of up to +/- 5% even with the current algorithm. In order to meaningfully react to a significant loss of hashing, we would need to consider a window of probably two weeks. However, it is not advisable to assume that the hashrate is constant in order to get a better measurement because the assumption is clearly false. Although difficulty target variations are not a big deal, the current Bitcoin algorithm fails here, with increasing hashrate. It issues coins faster than its assumed schedule. Therefore, errors need to be calculated, and retargeting can take these errors into account because no matter what, we will always be dealing with a finite sample.</summary>
    <published>2016-03-09T20:21:36+00:00</published>
  </entry>
</feed>
