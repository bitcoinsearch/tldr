<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>1</id>
  <title>Hardfork to fix difficulty drop algorithm</title>
  <updated>2023-06-11T04:15:51.738742+00:00</updated>
  <author>
    <name>Dave Hudson 2016-03-09 23:24:15</name>
  </author>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="0.9.0">python-feedgen</generator>
  <entry>
    <id>1</id>
    <title>Hardfork to fix difficulty drop algorithm</title>
    <updated>2023-06-11T04:15:51.738742+00:00</updated>
    <link href="https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-March/012545.html" rel="alternate"/>
    <summary>In an email exchange between Bob McElrath and Dave Hudson, they discuss the challenges of accurately measuring the hashrate in Bitcoin. Hudson suggests a damping-based design for measurement, but McElrath expresses doubts about its accuracy over short timeframes. Additionally, the consensus data lacks a strong notion of time, making it difficult to calculate difficulty based on anything outside of the consensus. However, McElrath notes that there is a wealth of literature available on the Poisson Point Process and temporal point process, which can be used to handle these issues. Hudson acknowledges that the measurement of hash rate is pretty inaccurate at best even with 2016 events, and we need to consider a window of probably two weeks to meaningfully react to a major loss of hashing. Although the assumption that hash rate is constant would give a better measurement, this assumption is clearly false. Any measurements taken over a period of even a few days have too much noise to be practically usable.</summary>
    <published>2016-03-09T23:24:15+00:00</published>
  </entry>
</feed>
