<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>1</id>
  <title>Reaching consensus on policy to continually increase block size limit as hardware improves, and a few other critical issues</title>
  <updated>2023-06-10T01:53:13.552645+00:00</updated>
  <author>
    <name>Michael Naber 2015-07-01 07:15:15</name>
  </author>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="0.9.0">python-feedgen</generator>
  <entry>
    <id>1</id>
    <title>Reaching consensus on policy to continually increase block size limit as hardware improves, and a few other critical issues</title>
    <updated>2023-06-10T01:53:13.552645+00:00</updated>
    <link href="https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-July/009293.html" rel="alternate"/>
    <summary>Adam agrees that the block size limit should be increased within the limits of technology, and continually so as hardware improves, in order to improve decentralization, throughput, and network reliability. However, there are some disagreements on important issues that are causing trouble for Bitcoin Core's goal of building the "best" globally aware global consensus network. One point of disagreement is whether fees and hash-rate will be high by limiting capacity, or if we need to limit capacity to have a "healthy fee market." Some argue that we can regulate the airline security requirements and set a minimum fee size for the sake of security, just like we can set the block limit.Another disagreement is whether every user should run a "full node" in order to use Bitcoin Core for better network reliability and validation of transaction integrity. It is argued that we don't need millions of nodes to participate in a broadcast network to ensure network reliability, and people only need to be able to easily validate the blockchain, not every time they use it.The third disagreement is whether Bitcoin Core should run as a high-memory server-grade software rather than for people's desktops. It is suggested that increasing capacity of Bitcoin Core matters when we can increase capacity by moving to hub and spoke/lightning, and we should stick to the goal of building the "best" globally aware globally consensus network.Adam suggests allocating some within tech limits bandwidth to decentralization improvements, and seeing work to improve decentralization with better pooling protocols that people are working on, to remove some of the artificial centralization in the system. He also proposes hypotheticals (unvalidated numbers) for different growth limiters. However, he warns against pushing things to the extreme level of centralization, which could weaken the security of the main Bitcoin chain and leave nothing secure to build on. Therefore, it's more appropriate for high scale to rely on lightning, or a semi-centralized trade-offs being in the side-chain model or similar, where the higher risk of centralization is opt-in and not exposed back to the Bitcoin network itself.The discussion revolves around the idea of scaling the Bitcoin network by increasing block size, either through a hard fork or side-chain. The potential benefits of such an experiment include faster transaction times and increased market signalling, but there are concerns about the limited scalability of simply doubling capacity. The group also discusses the importance of improving decentralization and analyzing bandwidth assumptions and game-theory in any proposal for scaling. It is noted that miners are currently imposing local policy limits on block size, and there may be value in mapping these limits to determine if the network is already at capacity.</summary>
    <published>2015-07-01T07:15:15+00:00</published>
  </entry>
</feed>
